{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"B1.2 (Different number of nodes).ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1BVmw85gZ0RNya2DzcpmwHnAiPI_cYGDq","authorship_tag":"ABX9TyP6SDB/R09IIIERvsAi/Q+E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"VjxeTyLwDOeN"},"source":["import numpy as np # linear algebra\r\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n","import matplotlib.pyplot as plt\r\n","import seaborn as sns\r\n","import keras\r\n","from keras.models import Sequential\r\n","from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Activation\r\n","from keras.optimizers import Adam\r\n","from keras.callbacks import TensorBoard\r\n","from matplotlib import pyplot as plt\r\n","from keras.utils.np_utils import to_categorical \r\n","from keras.preprocessing.image import ImageDataGenerator\r\n","from sklearn.model_selection import train_test_split\r\n","from tensorflow.keras.layers import BatchNormalization\r\n","import time\r\n","\r\n","\r\n","# ~ Data preprocessing section ~\r\n","# Save the training data from fashion-mnist_test.csv to the \"train\" DataFrame\r\n","train = pd.read_csv(\"drive/MyDrive/Colab Notebooks/fashion-mnist_train.csv\")\r\n","# Save the testing data from fashion-mnist_test.csv to the \"test\" DataFrame\r\n","test = pd.read_csv(\"drive/MyDrive/Colab Notebooks/fashion-mnist_test.csv\")\r\n","# From the training DataFrame, get the values of each label\r\n","Y_train = train['label'].values\r\n","# From the training DataFrame, drop the 'label' attribute\r\n","X_train = train.drop(labels = ['label'], axis = 1)\r\n","# From the testing DataFrame, get the values of each label\r\n","Y_test = test['label'].values\r\n","# From the testing DataFrame, drop the 'label' attribute\r\n","X_test = test.drop(labels = ['label'], axis = 1)\r\n","# Normalisation of our Dataframes (scale 0 to 1)\r\n","X_train = X_train / 255.0\r\n","X_test = X_test / 255.0\r\n","# Change of the shape of our DataFrames \r\n","X_train = X_train.values.reshape(-1,28,28,1)\r\n","X_test = X_test.values.reshape(-1,28,28,1)\r\n","# Convert to one-hot-encoding\r\n","Y_train = to_categorical(Y_train, num_classes = 10)\r\n","# Convert to one-hot-encoding\r\n","Y_test = to_categorical(Y_test, num_classes = 10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JzBszwoCDUzK"},"source":["# Define our 1st Convolutional Neural Network, with 1 Convolutional Layer\r\n","# In this section, we change the number of Nodes of each Convolutional Layer\r\n","# For more information about the CNN structure, check out our final report\r\n","name1 = '2_NN_128_256'\r\n","cnn_model_1 = Sequential([], name = name1)\r\n","cnn_model_1.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_1.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_1.add(BatchNormalization())\r\n","# 1st Convolutional Layer\r\n","cnn_model_1.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_1.add(BatchNormalization())\r\n","cnn_model_1.add(Flatten())\r\n","cnn_model_1.add(Dense(128, activation='relu'))\r\n","cnn_model_1.add(Dense(10, activation='softmax'))\r\n","\r\n","# Define our 2nd Convolutional Neural Network, with 3 Convolutional Layers\r\n","name2 = '3_NN_64_128_128'\r\n","cnn_model_2 = Sequential([], name = name2)\r\n","# 1st Convolutional Layer\r\n","cnn_model_2.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_2.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_2.add(BatchNormalization())\r\n","# 2nd Convolutional Layer\r\n","cnn_model_2.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_2.add(BatchNormalization())\r\n","# 3rd Convolutional Layer\r\n","cnn_model_2.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_2.add(Dense(128, activation='relu'))\r\n","cnn_model_2.add(Flatten())\r\n","cnn_model_2.add(Dense(128, activation='relu'))\r\n","cnn_model_2.add(Dense(10, activation='softmax'))\r\n","\r\n","# Define our 3rd Convolutional Neural Network, with 3 Convolutional Layers\r\n","name3 = '3_NN_64_128_256'\r\n","cnn_model_3 = Sequential([], name = name3)\r\n","# 1st Convolutional Layer\r\n","cnn_model_3.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_3.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_3.add(BatchNormalization())\r\n","# 2nd Convolutional Layer\r\n","cnn_model_3.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_3.add(BatchNormalization())\r\n","# 3rd Convolutional Layer\r\n","cnn_model_3.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_3.add(Dense(256, activation='relu'))\r\n","cnn_model_3.add(Flatten())\r\n","cnn_model_3.add(Dense(128, activation='relu'))\r\n","cnn_model_3.add(Dense(10, activation='softmax'))\r\n","\r\n","# Define our 4th Convolutional Neural Network, with 3 Convolutional Layers but diffrent number of nodes\r\n","name4 = '3_NN_128_128_256'\r\n","cnn_model_4 = Sequential([], name = name4)\r\n","# 1st Convolutional Layer\r\n","cnn_model_4.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_4.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_4.add(BatchNormalization())\r\n","# 2nd Convolutional Layer\r\n","cnn_model_4.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_4.add(BatchNormalization())\r\n","# 3rd Convolutional Layer\r\n","cnn_model_4.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_4.add(Dense(256, activation='relu'))\r\n","cnn_model_4.add(Flatten())\r\n","cnn_model_4.add(Dense(128, activation='relu'))\r\n","cnn_model_4.add(Dense(10, activation='softmax'))\r\n","\r\n","# Define our 5th Convolutional Neural Network, with 3 Convolutional Layers\r\n","name5 = '3_NN_128_256_256'\r\n","cnn_model_5 = Sequential([], name = name5)\r\n","# 1st Convolutional Layer\r\n","cnn_model_5.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_5.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_5.add(BatchNormalization())\r\n","# 2nd Convolutional Layer\r\n","cnn_model_5.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_5.add(BatchNormalization())\r\n","# 3rd Convolutional Layer\r\n","cnn_model_5.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_5.add(Dense(256, activation='relu'))\r\n","cnn_model_5.add(Flatten())\r\n","cnn_model_5.add(Dense(128, activation='relu'))\r\n","cnn_model_5.add(Dense(10, activation='softmax'))\r\n","\r\n","# Define our 6th Convolutional Neural Network, with 4 Convolutional Layers\r\n","name6 = \"4_NN_128_128_256_256\"\r\n","cnn_model_6 = Sequential([],name = name6)\r\n","# 1st Convolutional Layer\r\n","cnn_model_6.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_6.add(BatchNormalization())\r\n","# 2nd Convolutional Layer\r\n","cnn_model_6.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_6.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_6.add(BatchNormalization())\r\n","# 3rd Convolutional Layer\r\n","cnn_model_6.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_6.add(Dense(256, activation='relu'))\r\n","# 4th Convolutional Layer\r\n","cnn_model_6.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_6.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_6.add(BatchNormalization())\r\n","cnn_model_6.add(Flatten())\r\n","cnn_model_6.add(Dense(128, activation='relu'))\r\n","cnn_model_6.add(Dense(10, activation='softmax'))\r\n","\r\n","# Define our 7th Convolutional Neural Network, with 4 Convolutional Layers\r\n","name7 = \"4_NN_128_256_256_256\"\r\n","cnn_model_7 = Sequential([],name = name7)\r\n","# 1st Convolutional Layer\r\n","cnn_model_7.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_7.add(BatchNormalization())\r\n","# 2nd Convolutional Layer\r\n","cnn_model_7.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_7.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_7.add(BatchNormalization())\r\n","# 3rd Convolutional Layer\r\n","cnn_model_7.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_7.add(Dense(256, activation='relu'))\r\n","# 4th Convolutional Layer\r\n","cnn_model_7.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_7.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_7.add(BatchNormalization())\r\n","cnn_model_7.add(Flatten())\r\n","cnn_model_7.add(Dense(128, activation='relu'))\r\n","cnn_model_7.add(Dense(10, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Un8YACRODlsB"},"source":["cnn_models = [cnn_model_1,cnn_model_2,cnn_model_3,cnn_model_4,cnn_model_5,cnn_model_6,cnn_model_7]\r\n","\r\n","for model in cnn_models:\r\n","    # Get the summary for each diffrent CNN (Layer Type, Output Shape and parameters)\r\n","    model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hXbbbxupDnqF"},"source":["# Create a new dictionary\r\n","history_dict = {}\r\n","# Define our optimizer \r\n","optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\r\n","# Define our datagenerator object\r\n","datagen = ImageDataGenerator(\r\n","        featurewise_center=False,  # set input mean to 0 over the dataset\r\n","        samplewise_center=False,  # set each sample mean to 0\r\n","        featurewise_std_normalization=False,  # divide inputs by std of the dataset\r\n","        samplewise_std_normalization=False,  # divide each input by its std\r\n","        zca_whitening=False,  # dimesion reduction\r\n","        rotation_range=0.1,  # randomly rotate images in the range\r\n","        zoom_range = 0.1, # Randomly zoom image\r\n","        width_shift_range=0.1,  # randomly shift images horizontally\r\n","        height_shift_range=0.1,  # randomly shift images vertically\r\n","        horizontal_flip=False,  # randomly flip images\r\n","        vertical_flip=False)  # randomly flip images\r\n","# Generate data with the parameters given above, of our training DataFrame\r\n","datagen.fit(X_train)\r\n","# Initialize split method\r\n","X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state = 2)\r\n","# Compile our models and save the training information to the 'history_dict' dictionary\r\n","for model in cnn_models:\r\n","    model.compile(\r\n","    loss = 'categorical_crossentropy', optimizer = optimizer, metrics=['accuracy'])\r\n","    \r\n","    start = time.time()\r\n","    history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=128),\r\n","                              shuffle=True, epochs=50, \r\n","                              validation_data = (X_val, Y_val),verbose = 1, \r\n","                              steps_per_epoch=X_train.shape[0] // 128)\r\n","    history_dict[model.name] = history\r\n","    end = time.time()\r\n","# Print the time that every model did for training\r\n","    print(\"Time to fit the model : \" + str(end - start)+ \" seconds.\")\r\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L3mAPg-HDpI5"},"source":["# ~ Plotting Section ~ \r\n","plt.figure(1,figsize=(25,10))\r\n","# Initialize our counter\r\n","count = 1\r\n","# For every object in the 'history_dict' dictionary, plot 2 graphs in a subplot.\r\n","for history in history_dict:\r\n","    train_acc = history_dict[history].history['accuracy']\r\n","    val_acc = history_dict[history].history['val_accuracy']\r\n","    loss = history_dict[history].history['loss']\r\n","    val_loss = history_dict[history].history['val_loss']\r\n","\r\n","    plt.subplot(2,2,count)\r\n","    plt.plot(val_acc, label= \"Validation_accuracy \")\r\n","    plt.plot(train_acc, label = \"Train_accuracy \") \r\n","    plt.title(\"\"+history)\r\n","    plt.xlabel(\"Epochs\")\r\n","    plt.ylim(0,1)\r\n","    plt.xlim(0,50)\r\n","    plt.ylabel(\"Accuracy\")\r\n","    plt.legend()\r\n","\r\n","    plt.subplot(2,2,count+1)\r\n","    plt.subplots_adjust(hspace=0.4)\r\n","    plt.plot(val_loss, label=\"Validation_loss \")\r\n","    plt.plot(loss, label = \"Loss \")\r\n","    plt.title(\"\"+history)\r\n","    plt.xlabel(\"Epochs\")\r\n","    plt.ylim(0,1)\r\n","    plt.xlim(0,50)\r\n","    plt.ylabel(\"Loss\")\r\n","    plt.legend()\r\n","    count = count + 2\r\n","plt.show()\r\n","\r\n","# ~ Evaluation Section ~\r\n","# Get the evaluation of each model by using our testing DataFrame.\r\n","for model in cnn_models:\r\n","    score_model = model.evaluate(X_test, Y_test)\r\n","    #print(score2)\r\n","    print('Test loss:', score_model[0])\r\n","    print('Test accuracy:', score_model[1])"],"execution_count":null,"outputs":[]}]}