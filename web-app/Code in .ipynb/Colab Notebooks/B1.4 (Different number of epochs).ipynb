{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"B.1.4 (Different number of epochs).ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1YMADWDC3zLZYMHeBc1H-LFidOaHK7r1Q","authorship_tag":"ABX9TyPRaAwQ3QKxPI6VDpveCWXi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"28ZBXz7mCWYi"},"source":["import numpy as np\r\n","import pandas as pd\r\n","import matplotlib.pyplot as plt\r\n","import seaborn as sns\r\n","import keras\r\n","from keras.models import Sequential\r\n","from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Activation\r\n","from keras.optimizers import Adam\r\n","from keras.callbacks import TensorBoard\r\n","from matplotlib import pyplot as plt\r\n","from keras.utils.np_utils import to_categorical \r\n","from keras.preprocessing.image import ImageDataGenerator\r\n","from sklearn.model_selection import train_test_split\r\n","from tensorflow.keras.layers import BatchNormalization\r\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R2JaAobUNVyr"},"source":["# ~ Data preprocessing section ~\r\n","# Save the training data from fashion-mnist_test.csv to the \"train\" DataFrame\r\n","train = pd.read_csv(\"drive/MyDrive/Colab Notebooks/fashion-mnist_train.csv\")\r\n","# Save the testing data from fashion-mnist_test.csv to the \"test\" DataFrame\r\n","test = pd.read_csv(\"drive/MyDrive/Colab Notebooks/fashion-mnist_test.csv\")\r\n","# epx = Diffent number of epochs\r\n","epx = [10,25,50,75,100]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5sXG36fM1VA"},"source":["# In this section, we chose to train the best models with diffrent number of epochs\r\n","# We chose to keep 1 model per CNN type, 1 for a 1-Layer CNN, 1 for a 2-Layer CNN, 1 for a 3-Layer CNN, 1 for a 4-Layer CNN\r\n","\r\n","# Define the 1st Convolutional Neural Network, the best of our 1-Layer CNN tested so far.\r\n","name1 = '1_NN_256'\r\n","cnn_model_1 = Sequential([], name = name1)\r\n","# 1st Convolutional Layer\r\n","cnn_model_1.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_1.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_1.add(BatchNormalization())\r\n","cnn_model_1.add(Flatten())\r\n","cnn_model_1.add(Dense(128, activation='relu'))\r\n","cnn_model_1.add(Dense(10, activation='softmax'))\r\n","\r\n","# Define the 2nd Convolutional Neural Network, the best of our 2-Layer CNN tested so far.\r\n","name2 = '2_NN_128_128'\r\n","cnn_model_2 = Sequential([], name = name2)\r\n","# 1st Convolutional Layer\r\n","cnn_model_2.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_2.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_2.add(BatchNormalization())\r\n","# 2nd Convolutional Layer\r\n","cnn_model_2.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_2.add(BatchNormalization())\r\n","cnn_model_2.add(Flatten())\r\n","cnn_model_2.add(Dense(128, activation='relu'))\r\n","cnn_model_2.add(Dense(10, activation='softmax'))\r\n","\r\n","# Define the 3rd Convolutional Neural Network, the best of our 3-Layer CNN tested so far.\r\n","name3 = '3_NN_128_256_256'\r\n","cnn_model_3 = Sequential([], name = name3)\r\n","# 1st Convolutional Layer\r\n","cnn_model_3.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_3.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_3.add(BatchNormalization())\r\n","# 2nd Convolutional Layer\r\n","cnn_model_3.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_3.add(BatchNormalization())\r\n","# 3rd Convolutional Layer\r\n","cnn_model_3.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_3.add(Dense(256, activation='relu'))\r\n","cnn_model_3.add(Flatten())\r\n","cnn_model_3.add(Dense(128, activation='relu'))\r\n","cnn_model_3.add(Dense(10, activation='softmax'))\r\n","\r\n","# Define the 4th Convolutional Neural Network, the best of our 4-Layer CNN tested so far.\r\n","name4 = '4_NN_128_128_128_128'\r\n","cnn_model_4 = Sequential([],name = name4)\r\n","# 1st Convolutional Layer\r\n","cnn_model_4.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_4.add(BatchNormalization())\r\n","# 2nd Convolutional Layer\r\n","cnn_model_4.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_4.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_4.add(BatchNormalization())\r\n","# 3rd Convolutional Layer\r\n","cnn_model_4.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_4.add(Dense(128, activation='relu'))\r\n","# 4th Convolutional Layer\r\n","cnn_model_4.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_4.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_4.add(BatchNormalization())\r\n","cnn_model_4.add(Flatten())\r\n","cnn_model_4.add(Dense(128, activation='relu'))\r\n","cnn_model_4.add(Dense(10, activation='softmax'))\r\n","\r\n","\r\n","models = [cnn_model_1,cnn_model_2,cnn_model_3,cnn_model_4]\r\n","names = [name1, name2, name3, name4]\r\n","# Στα dictionaries models_accur, models_loss αποθηκέυουμε για κάθε μοντέλο \r\n","#τα αντιστοιχα test accur test loss για καθε αριθμό sample , \r\n","#δηλαδη models_accur = {1ΝΝ_256:[accur για 1000δειγματα,accur για 5000 δειγματα, κοκ ],2ΝΝ_64_128[...],...}\r\n","models_accur = {name1:[],name2:[],name3:[],name4:[]}\r\n","models_loss = {name1:[],name2:[],name3:[],name4:[]}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W16xhaw0N4SX"},"source":["# Define a dictionary\r\n","history_dict = {}\r\n","# With the following function, we train each model for a diffrent number of epochs \r\n","def bullet_4(model,name):\r\n","    for i in epx:\r\n","        # Data preprocessing ~ same as before\r\n","        Y_train = train['label'].values\r\n","        X_train = train.drop(labels = ['label'], axis = 1)\r\n","        Y_test = test['label'].values\r\n","        X_test = test.drop(labels = ['label'], axis = 1)\r\n","        X_train = X_train / 255.0\r\n","        X_test = X_test / 255.0\r\n","        X_train = X_train.values.reshape(-1,28,28,1)\r\n","        X_test = X_test.values.reshape(-1,28,28,1)\r\n","        # convert to one-hot-encoding\r\n","        Y_train = to_categorical(Y_train, num_classes = 10)\r\n","        # convert to one-hot-encoding\r\n","        Y_test = to_categorical(Y_test, num_classes = 10)\r\n","        # Define our optimizer\r\n","        optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\r\n","        # Compile the selected model\r\n","        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\r\n","        # Define our datagenerator object\r\n","        datagen = ImageDataGenerator(\r\n","            featurewise_center=False,  # set input mean to 0 over the dataset\r\n","            samplewise_center=False,  # set each sample mean to 0\r\n","            featurewise_std_normalization=False,  # divide inputs by std of the dataset\r\n","            samplewise_std_normalization=False,  # divide each input by its std\r\n","            zca_whitening=False,  # dimesion reduction\r\n","            rotation_range=0.1,  # randomly rotate images in the range\r\n","            zoom_range = 0.1, # Randomly zoom image\r\n","            width_shift_range=0.1,  # randomly shift images horizontally\r\n","            height_shift_range=0.1,  # randomly shift images vertically\r\n","            horizontal_flip=False,  # randomly flip images\r\n","            vertical_flip=False)  # randomly flip images\r\n","        # Generate data with the parameters given above, of our training DataFrame\r\n","        datagen.fit(X_train)\r\n","        # Initialize split method\r\n","        X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state = 2)\r\n","\r\n","        \r\n","        start=time.time()\r\n","        history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=128),\r\n","                                  shuffle=True, epochs=i, \r\n","                                  validation_data = (X_val, Y_val),verbose = 0, \r\n","                                  steps_per_epoch=X_train.shape[0] // 128)\r\n","        end=time.time()\r\n","        history_dict[model.name] = history\r\n","        # Evaluate the selected model\r\n","        score_model = model.evaluate(X_test, Y_test)\r\n","        print(i)\r\n","        # Print the evaluation results\r\n","        print('Test loss:', score_model[0])\r\n","        print('Test accuracy:', score_model[1])\r\n","        # Print the time that every model did for training\r\n","        print('Execution Time:',end-start)\r\n","        # Save the evaluation of each model\r\n","        models_accur[name].append(score_model[0])\r\n","        models_loss[name].append(score_model[1])\r\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ao6udbdJOz7z","outputId":"4d8721fb-c796-4fb6-8e9c-9909c73a7019"},"source":["# By calling the function, the training begins for every model and a diffrent number of epochs each time\r\n","for i in range(0,4):\r\n","    bullet_4(models[i],names[i])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["313/313 [==============================] - 1s 3ms/step - loss: 0.3067 - accuracy: 0.8897\n","10\n","Test loss: 0.30669087171554565\n","Test accuracy: 0.8896999955177307\n","Execution Time: 125.7446620464325\n","313/313 [==============================] - 1s 3ms/step - loss: 0.2843 - accuracy: 0.9063\n","25\n","Test loss: 0.2843060791492462\n","Test accuracy: 0.9063000082969666\n","Execution Time: 312.77123832702637\n","313/313 [==============================] - 1s 3ms/step - loss: 0.3146 - accuracy: 0.9137\n","50\n","Test loss: 0.31458187103271484\n","Test accuracy: 0.9136999845504761\n","Execution Time: 617.2789211273193\n","313/313 [==============================] - 1s 3ms/step - loss: 0.3394 - accuracy: 0.9151\n","75\n","Test loss: 0.3394123911857605\n","Test accuracy: 0.9150999784469604\n","Execution Time: 910.8562443256378\n","313/313 [==============================] - 1s 3ms/step - loss: 0.3633 - accuracy: 0.9194\n","100\n","Test loss: 0.3633403480052948\n","Test accuracy: 0.9193999767303467\n","Execution Time: 1189.2050967216492\n","313/313 [==============================] - 1s 3ms/step - loss: 0.2568 - accuracy: 0.9046\n","10\n","Test loss: 0.2568269670009613\n","Test accuracy: 0.9046000242233276\n","Execution Time: 118.93157386779785\n","313/313 [==============================] - 1s 3ms/step - loss: 0.3224 - accuracy: 0.8939\n","25\n","Test loss: 0.3223991096019745\n","Test accuracy: 0.8938999772071838\n","Execution Time: 295.1631588935852\n","313/313 [==============================] - 1s 3ms/step - loss: 0.2576 - accuracy: 0.9227\n","50\n","Test loss: 0.2576225697994232\n","Test accuracy: 0.9226999878883362\n","Execution Time: 591.6907415390015\n","313/313 [==============================] - 1s 3ms/step - loss: 0.3699 - accuracy: 0.9102\n","75\n","Test loss: 0.36986595392227173\n","Test accuracy: 0.9101999998092651\n","Execution Time: 876.2553451061249\n","313/313 [==============================] - 1s 3ms/step - loss: 0.3453 - accuracy: 0.9280\n","100\n","Test loss: 0.3453475832939148\n","Test accuracy: 0.9279999732971191\n","Execution Time: 1167.929552078247\n","313/313 [==============================] - 1s 4ms/step - loss: 0.2257 - accuracy: 0.9154\n","10\n","Test loss: 0.22570399940013885\n","Test accuracy: 0.9154000282287598\n","Execution Time: 147.14222383499146\n","313/313 [==============================] - 1s 4ms/step - loss: 0.2154 - accuracy: 0.9314\n","25\n","Test loss: 0.2153576910495758\n","Test accuracy: 0.9314000010490417\n","Execution Time: 359.5053081512451\n","313/313 [==============================] - 1s 4ms/step - loss: 0.2902 - accuracy: 0.9289\n","50\n","Test loss: 0.29020586609840393\n","Test accuracy: 0.9289000034332275\n","Execution Time: 715.2515773773193\n","313/313 [==============================] - 1s 4ms/step - loss: 0.3453 - accuracy: 0.9297\n","75\n","Test loss: 0.3452659547328949\n","Test accuracy: 0.9297000169754028\n","Execution Time: 1050.9880237579346\n","313/313 [==============================] - 1s 4ms/step - loss: 0.3134 - accuracy: 0.9372\n","100\n","Test loss: 0.313438355922699\n","Test accuracy: 0.9372000098228455\n","Execution Time: 1391.687852859497\n","313/313 [==============================] - 1s 4ms/step - loss: 0.2169 - accuracy: 0.9230\n","10\n","Test loss: 0.21692563593387604\n","Test accuracy: 0.9229999780654907\n","Execution Time: 143.88186621665955\n","313/313 [==============================] - 1s 4ms/step - loss: 0.2055 - accuracy: 0.9375\n","25\n","Test loss: 0.20554618537425995\n","Test accuracy: 0.9375\n","Execution Time: 359.6505677700043\n","313/313 [==============================] - 1s 4ms/step - loss: 0.2545 - accuracy: 0.9387\n","50\n","Test loss: 0.25453177094459534\n","Test accuracy: 0.9387000203132629\n","Execution Time: 718.2373049259186\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1LLqlHL9T1bu"},"source":["# ~ Plotting Section ~\r\n","# Plotting techniques same as before\r\n","plt.figure(1,figsize=(25,15))\r\n","ticks = [10,15,25,50]\r\n","for history in history_dict:\r\n","    train_acc = history_dict[history].history['accuracy']\r\n","    val_acc = history_dict[history].history['val_accuracy']\r\n","    loss = history_dict[history].history['loss']\r\n","    val_loss = history_dict[history].history['val_loss']\r\n","\r\n","    plt.subplot(2,1,1)\r\n","    plt.xticks(ticks=ticks)\r\n","    plt.title(\"Validation Loss results\")\r\n","    plt.plot(val_loss, label=\"\"+history)\r\n","    plt.xlabel(\"Epochs\")\r\n","    #plt.ylim(0,1)\r\n","    plt.xlim(0,5)\r\n","    plt.ylabel(\"Validation Loss\")\r\n","    plt.legend()\r\n","\r\n","    plt.subplot(2,1,2)\r\n","    plt.xticks(ticks=ticks)\r\n","    plt.title(\"Validation Loss results\")\r\n","    plt.plot(loss, label=\"\"+history)\r\n","    plt.xlabel(\"Epochs\")\r\n","    plt.xlim(0,5)\r\n","    plt.ylabel(\"Training Loss \")\r\n","    plt.legend()\r\n","plt.show()"],"execution_count":null,"outputs":[]}]}