{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"B.2.3 (Prediction Time of 1000 random samples).ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1_uklQY0KucZeJyDbp6dHcPG42HMmGULI","authorship_tag":"ABX9TyMHpnUxIIFPRtznDULpHnUB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"I82eEtWg0GpE"},"source":["import numpy as np\r\n","import pandas as pd\r\n","import matplotlib.pyplot as plt\r\n","import seaborn as sns\r\n","import keras\r\n","from keras.models import Sequential\r\n","from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Activation\r\n","from keras.optimizers import Adam\r\n","from keras.callbacks import TensorBoard\r\n","from matplotlib import pyplot as plt\r\n","from keras.utils.np_utils import to_categorical \r\n","from keras.preprocessing.image import ImageDataGenerator\r\n","from sklearn.model_selection import train_test_split\r\n","from tensorflow.keras.layers import BatchNormalization\r\n","import time\r\n","from sklearn.metrics import classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gbIRi0X_0xWm"},"source":["# In this section, we chose to train the best models and retrieve the time it takes to train a model so that it can predict 1000 images.\r\n","# We chose to keep 1 model per CNN type, 1 for a 1-Layer CNN, 1 for a 2-Layer CNN, 1 for a 3-Layer CNN, 1 for a 4-Layer CNN\r\n","\r\n","# Define the 1st Convolutional Neural Network, the best of our 1-Layer CNN tested so far.\r\n","name1 = '1_NN_256'\r\n","cnn_model_1 = Sequential([], name = name1)\r\n","# 1st Convolutional Layer\r\n","cnn_model_1.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_1.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_1.add(BatchNormalization())\r\n","cnn_model_1.add(Flatten())\r\n","cnn_model_1.add(Dense(128, activation='relu'))\r\n","cnn_model_1.add(Dense(10, activation='softmax'))\r\n","\r\n","# Define the 2nd Convolutional Neural Network, the best of our 2-Layer CNN tested so far.\r\n","name2 = '2_NN_128_128'\r\n","cnn_model_2 = Sequential([], name = name2)\r\n","# 1st Convolutional Layer\r\n","cnn_model_2.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_2.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_2.add(BatchNormalization())\r\n","# 2nd Convolutional Layer\r\n","cnn_model_2.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_2.add(BatchNormalization())\r\n","cnn_model_2.add(Flatten())\r\n","cnn_model_2.add(Dense(128, activation='relu'))\r\n","cnn_model_2.add(Dense(10, activation='softmax'))\r\n","\r\n","# Define the 3rd Convolutional Neural Network, the best of our 3-Layer CNN tested so far.\r\n","name3 = '3_NN_128_256_256'\r\n","cnn_model_3 = Sequential([], name = name3)\r\n","# 1st Convolutional Layer\r\n","cnn_model_3.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_3.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_3.add(BatchNormalization())\r\n","# 2nd Convolutional Layer\r\n","cnn_model_3.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_3.add(BatchNormalization())\r\n","# 3rd Convolutional Layer\r\n","cnn_model_3.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_3.add(Dense(256, activation='relu'))\r\n","cnn_model_3.add(Flatten())\r\n","cnn_model_3.add(Dense(128, activation='relu'))\r\n","cnn_model_3.add(Dense(10, activation='softmax'))\r\n","\r\n","# Define the 4th Convolutional Neural Network, the best of our 4-Layer CNN tested so far.\r\n","name4 = '4_NN_128_128_128_128'\r\n","cnn_model_4 = Sequential([],name = name4)\r\n","# 1st Convolutional Layer\r\n","cnn_model_4.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_4.add(BatchNormalization())\r\n","# 2nd Convolutional Layer\r\n","cnn_model_4.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_4.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_4.add(BatchNormalization())\r\n","# 3rd Convolutional Layer\r\n","cnn_model_4.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_4.add(Dense(128, activation='relu'))\r\n","# 4th Convolutional Layer\r\n","cnn_model_4.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\r\n","cnn_model_4.add(MaxPooling2D(pool_size=(2, 2)))\r\n","cnn_model_4.add(BatchNormalization())\r\n","cnn_model_4.add(Flatten())\r\n","cnn_model_4.add(Dense(128, activation='relu'))\r\n","cnn_model_4.add(Dense(10, activation='softmax'))\r\n","\r\n","# Edit the following 2 lines if you want to get the classification report for every model.\r\n","# cnn_models = [cnn_model_1,cnn_model_2,cnn_model_3,cnn_model_4]\r\n","cnn_models=[cnn_model_4]\r\n","names=[name1, name2, name3, name4]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LwuhKKvY0zNM"},"source":["# ~ Data preprocessing section ~\r\n","# Save the training data from fashion-mnist_test.csv to the \"train\" DataFrame\r\n","train = pd.read_csv(\"drive/MyDrive/Colab Notebooks/fashion-mnist_train.csv\")\r\n","# Save the testing data from fashion-mnist_test.csv to the \"test\" DataFrame\r\n","test = pd.read_csv(\"drive/MyDrive/Colab Notebooks/fashion-mnist_test.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xOBfyZSv05i_"},"source":["def bullet_1v2(model,fig_num):\r\n","    # ~ Data Preprocessing\r\n","    Y_train = train['label'].values\r\n","    X_train = train.drop(labels = ['label'], axis = 1)\r\n","    X_train = X_train / 255.0\r\n","    X_train = X_train.values.reshape(-1,28,28,1)\r\n","    Y_train = to_categorical(Y_train, num_classes = 10)\r\n","    # test0 refers to the 1000 random images, that will test our models\r\n","    test0=test.sample(n=1000)\r\n","    X_test = test0.drop(labels = ['label'], axis = 1)\r\n","    X_test = X_test / 255.0\r\n","    X_test = X_test.values.reshape(-1,28,28,1)\r\n","    \r\n","    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\r\n","    datagen = ImageDataGenerator(\r\n","            featurewise_center=False,  # set input mean to 0 over the dataset\r\n","            samplewise_center=False,  # set each sample mean to 0\r\n","            featurewise_std_normalization=False,  # divide inputs by std of the dataset\r\n","            samplewise_std_normalization=False,  # divide each input by its std\r\n","            zca_whitening=False,  # dimesion reduction\r\n","            rotation_range=0.1,  # randomly rotate images in the range\r\n","            zoom_range = 0.1, # Randomly zoom image\r\n","            width_shift_range=0.1,  # randomly shift images horizontally\r\n","            height_shift_range=0.1,  # randomly shift images vertically\r\n","            horizontal_flip=False,  # randomly flip images\r\n","            vertical_flip=False)  # randomly flip images\r\n","    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\r\n","    datagen.fit(X_train)\r\n","    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state = 2)\r\n","    history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=128),\r\n","                                  shuffle=True, epochs=2, \r\n","                                  validation_data = (X_val, Y_val),verbose = 1, \r\n","                                  steps_per_epoch=X_train.shape[0] // 128)\r\n","    \r\n","    ## Make predictions of classes\r\n","    start=time.time()\r\n","    predicted_classes = model.predict_classes(X_test)\r\n","    end=time.time()\r\n","    \r\n","    # Get true labels of test data \r\n","    y_true = test0.iloc[:, 0]\r\n","    # Convert to np array\r\n","    y_true=np.array(y_true)\r\n","    predicted_classes=np.array(predicted_classes)\r\n","    # Set name of classes \r\n","    target_names = [\"Class{}\".format(i) for i in range(10)]\r\n","    # Classification report to get precision recall f1 score support\r\n","    print(classification_report(y_true, predicted_classes, target_names=target_names))\r\n","    # Print the time the model needs to predict 1000 images\r\n","    print(\"Prediction of 1000 images time:\",end-start)\r\n","    \"\"\"\r\n","    Precision= Accuacy of positive predictions - What percent of your predictions were correct?\r\n","    Recall = Fraction of positives that were correctly indetified -What percent of the positive cases did you catch?\r\n","    F1 Score -What percent of positive predictions were correct?\r\n","    Support = the number of actual occurrences of the class in the specified dataset.\r\n","    \"\"\"\r\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BlWccz5R076j","executionInfo":{"status":"ok","timestamp":1610547072521,"user_tz":-120,"elapsed":68793,"user":{"displayName":"panos dallas","photoUrl":"","userId":"14803501364033350876"}},"outputId":"8ca41167-755c-4bef-f24b-d5473385aaaa"},"source":["fig_num=0\r\n","# By calling the function, the training begins for every model \r\n","for model in cnn_models:\r\n","    bullet_1v2(model,fig_num)\r\n","    fig_num+=1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[[[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  ...\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]]\n","\n","\n"," [[[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  ...\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]]\n","\n","\n"," [[[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  ...\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]]\n","\n","\n"," ...\n","\n","\n"," [[[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  ...\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]]\n","\n","\n"," [[[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  ...\n","\n","  [[0.06666667]\n","   [0.35686275]\n","   [0.36078431]\n","   ...\n","   [0.7372549 ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.56078431]\n","   [1.        ]\n","   [0.92941176]\n","   ...\n","   [0.43529412]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.03529412]\n","   [0.09411765]\n","   ...\n","   [0.62352941]\n","   [0.        ]\n","   [0.        ]]]\n","\n","\n"," [[[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  ...\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]\n","\n","  [[0.        ]\n","   [0.        ]\n","   [0.        ]\n","   ...\n","   [0.        ]\n","   [0.        ]\n","   [0.        ]]]]\n","(1000, 28, 28, 1)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/2\n","421/421 [==============================] - 37s 72ms/step - loss: 0.7034 - accuracy: 0.7453 - val_loss: 2.1072 - val_accuracy: 0.4398\n","Epoch 2/2\n","421/421 [==============================] - 29s 68ms/step - loss: 0.3756 - accuracy: 0.8611 - val_loss: 0.3078 - val_accuracy: 0.8897\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n","  warnings.warn('`model.predict_classes()` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","      Class0       0.91      0.85      0.88       106\n","      Class1       1.00      0.98      0.99        86\n","      Class2       0.88      0.74      0.80       114\n","      Class3       0.93      0.92      0.93       104\n","      Class4       0.84      0.81      0.83       107\n","      Class5       0.99      0.98      0.99       104\n","      Class6       0.65      0.78      0.71        91\n","      Class7       0.90      0.99      0.95        96\n","      Class8       0.87      0.99      0.92        93\n","      Class9       0.98      0.91      0.94        99\n","\n","    accuracy                           0.89      1000\n","   macro avg       0.90      0.89      0.89      1000\n","weighted avg       0.90      0.89      0.89      1000\n","\n","Prediction of 1000 images time: 0.43497657775878906\n"],"name":"stdout"}]}]}